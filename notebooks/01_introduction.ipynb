{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Necessity and Sufficiency Analysis\n",
    "\n",
    "Welcome! This notebook introduces the key concepts behind evaluating XAI robustness using necessity and sufficiency scores.\n",
    "\n",
    "## What This Framework Does\n",
    "\n",
    "This framework helps answer: **\"Are XAI methods (LIME, SHAP) truly identifying important features?\"**\n",
    "\n",
    "We use two causal concepts:\n",
    "1. **Necessity**: Is the feature essential for the prediction?\n",
    "2. **Sufficiency**: Can the feature drive predictions on its own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### Necessity\n",
    "\n",
    "**Question**: \"If I change this feature, will the model's prediction change?\"\n",
    "\n",
    "**High Necessity** = Changing the feature consistently flips the prediction\n",
    "\n",
    "**Example**: \n",
    "- Model predicts \"Malignant tumor\" (Class 1)\n",
    "- Change \"tumor size\" from large to small\n",
    "- Prediction changes to \"Benign\" (Class 0)\n",
    "- → Tumor size is NECESSARY for the malignant prediction\n",
    "\n",
    "### Sufficiency\n",
    "\n",
    "**Question**: \"Can this feature alone produce the target prediction?\"\n",
    "\n",
    "**High Sufficiency** = Setting the feature to a specific value reliably produces the outcome\n",
    "\n",
    "**Example**:\n",
    "- Model predicts \"Benign tumor\" (Class 0)\n",
    "- Set \"tumor size\" to very large\n",
    "- Prediction changes to \"Malignant\" (Class 1)\n",
    "- → Large tumor size is SUFFICIENT for malignant prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Intuition\n",
    "\n",
    "![Concept Diagram](../images/concept_diagram.png)\n",
    "\n",
    "### Interpreting Scores\n",
    "\n",
    "| Necessity | Sufficiency | Interpretation |\n",
    "|-----------|-------------|----------------|\n",
    "| High | High | **Crucial feature** - Essential AND can drive decisions |\n",
    "| High | Low | **Necessary but not sufficient** - Important but needs other features |\n",
    "| Low | High | **Sufficient but not necessary** - Can drive decisions but alternatives exist |\n",
    "| Low | Low | **Not important** - Neither essential nor driving decisions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters for XAI\n",
    "\n",
    "### The Problem\n",
    "\n",
    "LIME and SHAP rank features as \"important\", but:\n",
    "- Do they agree with each other?\n",
    "- Are these features truly causal?\n",
    "- Can we trust the rankings?\n",
    "\n",
    "### Our Solution\n",
    "\n",
    "**Hypothesis**: If a feature is truly important, it should be:\n",
    "1. **Necessary** (changing it affects predictions)\n",
    "2. **Sufficient** (it can drive predictions)\n",
    "\n",
    "**Robustness Test**: Do top-ranked features have high necessity and sufficiency?\n",
    "\n",
    "![Robustness Concept](../images/robustness_concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Example\n",
    "\n",
    "Let's see this in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate a simple scenario\n",
    "# Feature X2 is important, X1 is not\n",
    "\n",
    "# Imagine we calculated these scores:\n",
    "features = ['X1 (irrelevant)', 'X2 (important)', 'X3 (moderate)']\n",
    "necessity = [0.05, 0.85, 0.45]\n",
    "sufficiency = [0.03, 0.72, 0.38]\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, necessity, width, label='Necessity', color='#FF6B6B', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, sufficiency, width, label='Sufficiency', color='#4ECDC4', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Example: Feature Importance via Necessity & Sufficiency', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(features)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "               f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- X1: Low scores → Not important (as expected)\")\n",
    "print(\"- X2: High scores → Very important and causal\")\n",
    "print(\"- X3: Moderate scores → Somewhat important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Framework Workflow\n",
    "\n",
    "```\n",
    "1. Train Model\n",
    "   ↓\n",
    "2. Calculate Global Necessity & Sufficiency\n",
    "   - For each feature\n",
    "   - Across many instances\n",
    "   - Using forward counterfactuals\n",
    "   ↓\n",
    "3. Get LIME/SHAP Rankings\n",
    "   - For test instances\n",
    "   - Extract top-k features\n",
    "   ↓\n",
    "4. Evaluate Robustness\n",
    "   - Do top ranks have high necessity?\n",
    "   - Do top ranks have high sufficiency?\n",
    "   - Is the pattern monotonic?\n",
    "   ↓\n",
    "5. Visualize & Interpret\n",
    "   - Global scores\n",
    "   - Robustness curves\n",
    "   - Comparison between methods\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Advantages\n",
    "\n",
    "### 1. No Causal Model Needed\n",
    "- Traditional causal methods require domain knowledge\n",
    "- Our approach works with any trained model\n",
    "\n",
    "### 2. Works on Complex Data\n",
    "- High-dimensional features\n",
    "- Sparse datasets\n",
    "- Imbalanced classes\n",
    "\n",
    "### 3. Model-Agnostic\n",
    "- Logistic Regression\n",
    "- Random Forests\n",
    "- Neural Networks\n",
    "- Any classifier!\n",
    "\n",
    "### 4. Principled Evaluation\n",
    "- Grounded in causal theory\n",
    "- Validated on synthetic data\n",
    "- Published methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "\n",
    "### Healthcare\n",
    "- Validate feature importance in disease prediction\n",
    "- Ensure critical symptoms are identified\n",
    "- Compare diagnostic models\n",
    "\n",
    "### Finance\n",
    "- Credit scoring model validation\n",
    "- Risk factor identification\n",
    "- Regulatory compliance\n",
    "\n",
    "### Geophysics (Original Application)\n",
    "- Hydrocarbon prospect evaluation\n",
    "- Seismic indicator assessment\n",
    "- Drilling decision support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the concepts, explore:\n",
    "\n",
    "1. **02_toy_example.ipynb**\n",
    "   - See validation with logical operators\n",
    "   - Verify scores match theory\n",
    "\n",
    "2. **03_full_analysis.ipynb**\n",
    "   - Complete walkthrough on real data\n",
    "   - Step-by-step analysis\n",
    "\n",
    "3. **Run Quick Demo**\n",
    "   ```bash\n",
    "   cd ../src\n",
    "   python demo.py\n",
    "   ```\n",
    "\n",
    "4. **Full Analysis**\n",
    "   ```bash\n",
    "   python main.py --dataset breast_cancer --model logistic\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Papers Implemented\n",
    "\n",
    "1. **Chowdhury et al. (2023)**\n",
    "   - \"Explaining Explainers: Necessity and Sufficiency in Tabular Data\"\n",
    "   - NeurIPS 2023 Workshop on Table Representation Learning\n",
    "\n",
    "2. **Chowdhury et al. (2025)**\n",
    "   - \"A unified framework for evaluating the robustness of machine-learning interpretability\"\n",
    "   - Geophysics, Vol. 90, No. 3, pp. IM103-IM118\n",
    "\n",
    "### Theoretical Foundations\n",
    "\n",
    "- **Pearl (2009)**: Causality - Models, Reasoning and Inference\n",
    "- **Halpern (2016)**: Actual Causality\n",
    "- **Swartz (1997)**: The Concepts of Necessary and Sufficient Conditions\n",
    "\n",
    "### XAI Methods\n",
    "\n",
    "- **LIME**: Ribeiro et al. (2016) - \"Why Should I Trust You?\"\n",
    "- **SHAP**: Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "For more details:\n",
    "- See `../docs/methodology.md` for complete methodology\n",
    "- Check `../README.md` for usage instructions\n",
    "- Read the papers for theoretical background\n",
    "- Open an issue on GitHub for support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
